{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.warehouse.dir\",\"file:///c:/tmp\")\\\n",
    "    .appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "sc = spark.sparkContext\n",
    "raw_data =sc.textFile('kddcup.data_10_percent.gz').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(1) kddcup.data_10_percent.gz MapPartitionsRDD[1] at textFile at <unknown>:0 [Memory Serialized 1x Replicated]\\n |  kddcup.data_10_percent.gz HadoopRDD[0] at textFile at <unknown>:0 [Memory Serialized 1x Replicated]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.toDebugString()#可以看到rdd之间是如何转换的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda l:l.split(\",\"))\n",
    "#csv_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data = csv_data.map(lambda  p:Row(\n",
    "    duration =int(p[0]),\n",
    "    protocol_type =p[1],\n",
    "    service =p[2],\n",
    "    flag =p[3],\n",
    "    src_bytes =int(p[4]),\n",
    "    dst_bytes =int(p[5]))                       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df =spark.createDataFrame(row_data)\n",
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查询tcp 超过1秒的，同时没有数据传输的\n",
    "tcp_interactions = spark.sql(\"select duration,dst_bytes from interactions where protocol_type ='tcp' and duration >1000 and dst_bytes =0 \")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|190065|\n",
      "|          udp| 20354|\n",
      "|         icmp|283602|\n",
      "+-------------+------+\n",
      "\n",
      "querytime  in 10.942 seconds\n"
     ]
    }
   ],
   "source": [
    "#按照协议类型进行分组统计\n",
    "from time import time\n",
    "t0 =time()\n",
    "interactions_df.select(\"protocol_type\",\"duration\",\"dst_bytes\").groupBy(\"protocol_type\").count().show()\n",
    "tt =time()-t0\n",
    "print(\"querytime  in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|protocol_type|count|\n",
      "+-------------+-----+\n",
      "|          tcp|  139|\n",
      "+-------------+-----+\n",
      "\n",
      "querytime  in 16.074 seconds\n"
     ]
    }
   ],
   "source": [
    "#按照协议类型进行分组统计\n",
    "from time import time\n",
    "t0 =time()\n",
    "interactions_df.select(\"protocol_type\",\"duration\",\"dst_bytes\")\\\n",
    "    .filter((interactions_df.dst_bytes ==0)&(interactions_df.duration >1000))\\\n",
    "    .groupBy(\"protocol_type\").count().show()\n",
    "tt =time()-t0\n",
    "print(\"querytime  in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_type(label):\n",
    "    if label!=\"normal\":\n",
    "        return \"attack\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "    \n",
    "raw_labeled_data =csv_data.map(lambda p:Row(\n",
    "    duration =int(p[0]),\n",
    "    protocol_type =p[1],\n",
    "    service =p[2],\n",
    "    flag =p[3],\n",
    "    src_bytes =int(p[4]),\n",
    "    dst_bytes =int(p[5]),\n",
    "    label =get_label_type(p[41]))\n",
    ")\n",
    "interactions_labeled_df =spark.createDataFrame(raw_labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| label| count|\n",
      "+------+------+\n",
      "|attack|494021|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_labeled_df.select(\"label\").groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
